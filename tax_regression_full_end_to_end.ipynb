{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cd3f4bc",
   "metadata": {},
   "source": [
    "# Tax Liability Prediction (Regression) â€” End-to-End ML Notebook ðŸ§¾ðŸ“ˆ\n",
    "\n",
    "This is a complete **industry-style** machine learning notebook using the synthetic tax dataset.\n",
    "\n",
    "## ðŸŽ¯ Objective\n",
    "Predict **`TaxLiability`** (a continuous numeric value) using demographic, income, and deduction-related features.\n",
    "\n",
    "## âœ… Whatâ€™s included\n",
    "- Data loading + sanity checks\n",
    "- **Advanced EDA (multi-level)**\n",
    "- Cleaning + preprocessing (numeric & categorical)\n",
    "- **Feature engineering**\n",
    "- Baseline + **6+ regression models**\n",
    "- Evaluation with **RMSE, MAE, RÂ², MAPE**\n",
    "- Cross-validation (CV) comparison\n",
    "- Residual diagnostics (pred vs actual, residual plots)\n",
    "- Permutation importance (model-agnostic)\n",
    "- Save best model pipeline for deployment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb59cd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 0) Setup\n",
    "# ============================\n",
    "DATA_PATH = \"tax_synthetic_ml_dataset.csv\"  # keep CSV in same folder as this notebook\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "pd.set_option(\"display.width\", 140)\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a904ae99",
   "metadata": {},
   "source": [
    "## 1) Data Audit (Quality Checks)\n",
    "We check:\n",
    "- types\n",
    "- missing values\n",
    "- duplicates\n",
    "- summary statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b66555",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538d731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values (should be none for this synthetic dataset, but we still check)\n",
    "missing = df.isna().sum().sort_values(ascending=False)\n",
    "missing = missing[missing > 0]\n",
    "print(\"Columns with missing:\", len(missing))\n",
    "missing.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aec1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duplicates\n",
    "dup = df.duplicated().sum()\n",
    "print(\"Duplicate rows:\", dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb112c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=\"all\").T.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fac4b4",
   "metadata": {},
   "source": [
    "## 2) Define Regression Target\n",
    "### Target: `TaxLiability`\n",
    "We drop classification-only label `AuditFlag`.\n",
    "We also drop `EffectiveTaxRate` because it is derived from the target (`TaxLiability / TotalIncome`), which would cause leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35198c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET = \"TaxLiability\"\n",
    "\n",
    "drop_cols = [\"AuditFlag\", \"EffectiveTaxRate\"]\n",
    "df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET].astype(float)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"Target range:\", (y.min(), y.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4f2da7",
   "metadata": {},
   "source": [
    "## 3) Advanced EDA (Level 1): Target Distribution & Skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf052e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "y.hist(bins=60)\n",
    "plt.title(\"Target Distribution: TaxLiability\")\n",
    "plt.xlabel(\"TaxLiability\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Skewness:\", float(y.skew()))\n",
    "print(\"Kurtosis:\", float(y.kurtosis()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ab66be",
   "metadata": {},
   "source": [
    "## 4) Advanced EDA (Level 2): Numeric Distributions & Outliers\n",
    "We inspect distributions of major numeric variables and identify potential outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1eec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols_all = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols_all = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "major_num = [\"Age\",\"AnnualIncome\",\"BusinessIncome\",\"CapitalGains\",\"DeductionsTotal\",\"TaxableIncome\"]\n",
    "major_num = [c for c in major_num if c in X.columns]\n",
    "\n",
    "for col in major_num:\n",
    "    plt.figure()\n",
    "    X[col].hist(bins=50)\n",
    "    plt.title(f\"Distribution: {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "# Quick IQR outlier count (informative only)\n",
    "def iqr_outlier_count(s):\n",
    "    q1, q3 = np.percentile(s.dropna(), [25, 75])\n",
    "    iqr = q3 - q1\n",
    "    lo, hi = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "    return int(((s < lo) | (s > hi)).sum())\n",
    "\n",
    "outlier_counts = {c: iqr_outlier_count(X[c]) for c in major_num}\n",
    "pd.Series(outlier_counts).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0198714e",
   "metadata": {},
   "source": [
    "## 5) Advanced EDA (Level 3): Relationships with Target\n",
    "- Scatter plots for numeric features\n",
    "- Target mean by categorical features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524f1654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots: numeric vs target (sample for speed/clarity)\n",
    "sample_idx = X.sample(n=min(6000, len(X)), random_state=42).index\n",
    "\n",
    "for col in major_num:\n",
    "    plt.figure()\n",
    "    plt.scatter(X.loc[sample_idx, col], y.loc[sample_idx], alpha=0.25)\n",
    "    plt.title(f\"{col} vs TaxLiability\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"TaxLiability\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5ed361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical: average TaxLiability by category\n",
    "for col in cat_cols_all:\n",
    "    stats = df.groupby(col)[TARGET].agg([\"mean\",\"median\",\"count\"]).sort_values(\"mean\", ascending=False)\n",
    "    display(stats.head(10))\n",
    "    \n",
    "    plt.figure()\n",
    "    stats[\"mean\"].head(12).plot(kind=\"bar\")\n",
    "    plt.title(f\"Mean TaxLiability by {col} (Top 12)\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Mean TaxLiability\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b11cc5",
   "metadata": {},
   "source": [
    "## 6) Advanced EDA (Level 4): Correlation (Numeric)\n",
    "Correlation is useful for redundancy detection and feature selection insight.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d7436f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df[num_cols_all + [TARGET]].corr(numeric_only=True)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(corr.values, aspect=\"auto\")\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
    "plt.yticks(range(len(corr.index)), corr.index)\n",
    "plt.title(\"Correlation Heatmap (Numeric)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17d8bed",
   "metadata": {},
   "source": [
    "## 7) Feature Engineering (Professional)\n",
    "We create additional meaningful features:\n",
    "- `TotalIncome` = Annual + Business + CapitalGains\n",
    "- `BusinessIncomeRatio` = Business / (Annual + 1)\n",
    "- `DeductionsRatio` = DeductionsTotal / (TotalIncome + 1)\n",
    "- `IncomePerDependent` = TotalIncome / (Dependents + 1)\n",
    "- `LogTotalIncome` = log1p(TotalIncome) to reduce skewness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c084ab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe = df.copy()\n",
    "\n",
    "df_fe[\"TotalIncome\"] = df_fe[\"AnnualIncome\"] + df_fe[\"BusinessIncome\"] + df_fe[\"CapitalGains\"]\n",
    "df_fe[\"BusinessIncomeRatio\"] = df_fe[\"BusinessIncome\"] / (df_fe[\"AnnualIncome\"] + 1.0)\n",
    "df_fe[\"DeductionsRatio\"] = df_fe[\"DeductionsTotal\"] / (df_fe[\"TotalIncome\"] + 1.0)\n",
    "df_fe[\"IncomePerDependent\"] = df_fe[\"TotalIncome\"] / (df_fe[\"Dependents\"] + 1.0)\n",
    "df_fe[\"LogTotalIncome\"] = np.log1p(df_fe[\"TotalIncome\"])\n",
    "\n",
    "X = df_fe.drop(columns=[TARGET])\n",
    "y = df_fe[TARGET].astype(float)\n",
    "\n",
    "num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7579bead",
   "metadata": {},
   "source": [
    "## 8) Train/Validation/Test Split\n",
    "We use:\n",
    "- Train (70%)\n",
    "- Validation (15%)\n",
    "- Test (15%)\n",
    "\n",
    "Validation helps model selection without touching the final test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1462b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=42)\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36d0acd",
   "metadata": {},
   "source": [
    "## 9) Preprocessing Pipeline\n",
    "- Numeric: median imputation + scaling\n",
    "- Categorical: most_frequent imputation + one-hot encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d7f7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "num_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(transformers=[\n",
    "    (\"num\", num_pipe, num_cols),\n",
    "    (\"cat\", cat_pipe, cat_cols),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b7d114",
   "metadata": {},
   "source": [
    "## 10) Evaluation Metrics (Regression)\n",
    "We compute:\n",
    "- **RMSE** (lower is better)\n",
    "- **MAE** (lower is better)\n",
    "- **RÂ²** (higher is better)\n",
    "- **MAPE** (lower is better; careful when yâ‰ˆ0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cece393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def regression_metrics(y_true, y_pred):\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    # avoid division by tiny values for MAPE\n",
    "    denom = np.clip(np.abs(y_true), 1.0, None)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / denom)) * 100\n",
    "    return {\"RMSE\": rmse, \"MAE\": mae, \"R2\": r2, \"MAPE_%\": mape}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370fb5de",
   "metadata": {},
   "source": [
    "## 11) Train 6+ Models (Baseline to Advanced)\n",
    "Models:\n",
    "1. Linear Regression (baseline)\n",
    "2. Ridge\n",
    "3. Lasso\n",
    "4. ElasticNet\n",
    "5. RandomForestRegressor\n",
    "6. ExtraTreesRegressor\n",
    "7. HistGradientBoostingRegressor\n",
    "\n",
    "We compare on the **validation set**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1330a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, HistGradientBoostingRegressor\n",
    "\n",
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(alpha=1.0, random_state=42),\n",
    "    \"Lasso\": Lasso(alpha=0.0008, random_state=42, max_iter=20000),\n",
    "    \"ElasticNet\": ElasticNet(alpha=0.001, l1_ratio=0.35, random_state=42, max_iter=20000),\n",
    "    \"RandomForest\": RandomForestRegressor(n_estimators=400, random_state=42, n_jobs=-1),\n",
    "    \"ExtraTrees\": ExtraTreesRegressor(n_estimators=600, random_state=42, n_jobs=-1),\n",
    "    \"HistGradientBoosting\": HistGradientBoostingRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "val_results = []\n",
    "trained = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline(steps=[(\"preprocess\", preprocess), (\"model\", model)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    pred_val = pipe.predict(X_val)\n",
    "    m = regression_metrics(y_val, pred_val)\n",
    "    m[\"Model\"] = name\n",
    "    val_results.append(m)\n",
    "    trained[name] = pipe\n",
    "    print(name, m)\n",
    "\n",
    "val_df = pd.DataFrame(val_results).set_index(\"Model\").sort_values([\"RMSE\",\"MAE\"])\n",
    "val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72112ed3",
   "metadata": {},
   "source": [
    "## 12) Cross-Validation (CV) Score for Top Models\n",
    "We use 5-fold CV RMSE to confirm stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a046a04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "top_models = val_df.head(4).index.tolist()\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "cv_rows = []\n",
    "for name in top_models:\n",
    "    pipe = trained[name]\n",
    "    # negative RMSE (scikit returns negative for loss metrics)\n",
    "    scores = cross_val_score(pipe, X_train, y_train, scoring=\"neg_root_mean_squared_error\", cv=cv, n_jobs=-1)\n",
    "    cv_rows.append({\n",
    "        \"Model\": name,\n",
    "        \"CV_RMSE_mean\": (-scores).mean(),\n",
    "        \"CV_RMSE_std\": (-scores).std()\n",
    "    })\n",
    "\n",
    "cv_df = pd.DataFrame(cv_rows).set_index(\"Model\").sort_values(\"CV_RMSE_mean\")\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389cc651",
   "metadata": {},
   "source": [
    "## 13) Final Model Selection + Test Evaluation\n",
    "We pick the best model by **validation RMSE**, refit on **Train+Val**, then evaluate on **Test**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4b5097",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_name = val_df.index[0]\n",
    "print(\"Best model by validation RMSE:\", best_name)\n",
    "\n",
    "best_pipe = Pipeline(steps=[(\"preprocess\", preprocess), (\"model\", models[best_name])])\n",
    "\n",
    "# Refit on Train+Val\n",
    "X_trval = pd.concat([X_train, X_val], axis=0)\n",
    "y_trval = pd.concat([y_train, y_val], axis=0)\n",
    "\n",
    "best_pipe.fit(X_trval, y_trval)\n",
    "\n",
    "pred_test = best_pipe.predict(X_test)\n",
    "test_metrics = regression_metrics(y_test, pred_test)\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bd843e",
   "metadata": {},
   "source": [
    "## 14) Diagnostics (Professional Plots)\n",
    "- Predicted vs Actual\n",
    "- Residual distribution\n",
    "- Residuals vs Predicted (heteroscedasticity check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f986b55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted vs Actual\n",
    "plt.figure()\n",
    "plt.scatter(y_test, pred_test, alpha=0.3)\n",
    "plt.title(f\"Predicted vs Actual (Test) â€” {best_name}\")\n",
    "plt.xlabel(\"Actual TaxLiability\")\n",
    "plt.ylabel(\"Predicted TaxLiability\")\n",
    "plt.show()\n",
    "\n",
    "# Residuals\n",
    "resid = y_test - pred_test\n",
    "\n",
    "plt.figure()\n",
    "resid.hist(bins=60)\n",
    "plt.title(\"Residual Distribution (Test)\")\n",
    "plt.xlabel(\"Residual (Actual - Pred)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(pred_test, resid, alpha=0.3)\n",
    "plt.axhline(0, color=\"red\")\n",
    "plt.title(\"Residuals vs Predicted (Test)\")\n",
    "plt.xlabel(\"Predicted TaxLiability\")\n",
    "plt.ylabel(\"Residual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a69d7",
   "metadata": {},
   "source": [
    "## 15) Permutation Importance (Model-Agnostic)\n",
    "Shows which input features matter most (works with the full pipeline).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a145c7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Use a subset for speed\n",
    "X_imp = X_test.sample(n=min(5000, len(X_test)), random_state=42)\n",
    "y_imp = y_test.loc[X_imp.index]\n",
    "\n",
    "r = permutation_importance(best_pipe, X_imp, y_imp, n_repeats=5, random_state=42, n_jobs=-1, scoring=\"neg_root_mean_squared_error\")\n",
    "\n",
    "# Build final feature names after OHE\n",
    "ohe = best_pipe.named_steps[\"preprocess\"].named_transformers_[\"cat\"].named_steps[\"ohe\"]\n",
    "cat_feature_names = ohe.get_feature_names_out(cat_cols).tolist()\n",
    "feature_names = num_cols + cat_feature_names\n",
    "\n",
    "imp_df = pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"importance_mean\": r.importances_mean,\n",
    "    \"importance_std\": r.importances_std\n",
    "}).sort_values(\"importance_mean\", ascending=False)\n",
    "\n",
    "imp_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3064da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top 20 importances\n",
    "topk = imp_df.head(20).iloc[::-1]\n",
    "\n",
    "plt.figure(figsize=(8, 7))\n",
    "plt.barh(topk[\"feature\"], topk[\"importance_mean\"])\n",
    "plt.title(\"Top 20 Permutation Importances (RMSE impact)\")\n",
    "plt.xlabel(\"Importance (mean)\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d1170c",
   "metadata": {},
   "source": [
    "## 16) Save Final Pipeline (Deployment-Ready)\n",
    "This `.joblib` contains preprocessing + model in one object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b582bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "ARTIFACT_PATH = \"tax_liability_regression_pipeline.joblib\"\n",
    "joblib.dump(best_pipe, ARTIFACT_PATH)\n",
    "print(\"Saved:\", ARTIFACT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977d041e",
   "metadata": {},
   "source": [
    "## 17) Quick Inference Example\n",
    "How to predict on a single new sample (dictionary â†’ DataFrame â†’ predict).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f59f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_row = X_test.iloc[[0]].copy()\n",
    "pred_example = float(best_pipe.predict(example_row)[0])\n",
    "print(\"Example prediction TaxLiability:\", pred_example)\n",
    "example_row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d501d44",
   "metadata": {},
   "source": [
    "## Conclusion âœ…\n",
    "You built a full **regression** ML pipeline and saved a deployment-ready artifact.\n",
    "\n",
    "**Best model:** chosen using validation RMSE  \n",
    "**Final evaluation:** RMSE / MAE / RÂ² / MAPE on test set  \n",
    "**Explain in viva:** \"We transformed the tax dataset into regression to forecast tax liability, using engineered income & deductions features and comparing multiple models with RMSE/MAE/RÂ².\"\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
